# ðŸ§  Transformer from Scratch in PyTorch

This project is a full implementation of the Transformer architecture, as introduced in the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper, built entirely from scratch using PyTorch â€” without relying on high-level libraries like Hugging Face Transformers.

## ðŸš€ Overview

The goal of this project is to gain a deep understanding of the internal mechanics of the Transformer model by implementing each component step by step, including:

- Positional Encoding
- Multi-Head Self-Attention
- Scaled Dot-Product Attention
- Layer Normalization
- Residual Connections
- Decoder only transformer implementation

